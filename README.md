# Resume Bias Analysis in Data Science

## ðŸŒŸ ProjektÃ¼bersicht

Dieses Projekt untersucht mÃ¶gliche **Biases** in LebenslÃ¤ufen (Resumes), insbesondere solche, die auf Geschlecht, Ausdrucksweise oder Stil basieren kÃ¶nnten â€” mit Fokus auf Rollen im Bereich Data Science. Ziel ist es, systematische Muster zu erkennen, die in automatisierten Auswahlverfahren zu Verzerrungen fÃ¼hren kÃ¶nnten, und Handlungsempfehlungen ableiten zu kÃ¶nnen.

---

## ðŸ“ Verzeichnisstruktur

```

â”œâ”€â”€ data
â”‚   â”œâ”€â”€ processed
â”‚   â”‚   â”œâ”€â”€ exports
â”‚   â”‚   â”œâ”€â”€ images
â”‚   â”‚   â””â”€â”€ applicants_with_demographics.csv
â”‚   â””â”€â”€ raw
â”‚       â””â”€â”€ AI_Resume_Screening.csv
â”œâ”€â”€ notebooks
â”‚   â”œâ”€â”€ bias_analysis.ipynb
â”‚   â”œâ”€â”€ datascience.ipynb
â”‚   â”œâ”€â”€ fairness_metrics.py
â”‚   â”œâ”€â”€ fairness_quickstart.py
â”‚   â””â”€â”€ openai_test.ipynb
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ structure.py
```

---

## âœ… Voraussetzungen & Installation

Folge diesen Schritten, um das Projekt lokal lauffÃ¤hig zu machen:

```bash
# 1. Repository klonen
git clone https://github.com/spasegirl/resume_bias_analysis_datascience.git
cd resume_bias_analysis_datascience

# 2. Virtuelle Umgebung einrichten (optional, aber empfohlen)
python3 -m venv venv
source venv/bin/activate      # auf macOS / Linux
# venv\Scripts\activate       # auf Windows

# 3. AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt

# 4. Jupyter Notebook starten
jupyter notebook
```

Stelle sicher, dass alle Pakete wie `pandas`, `numpy`, `scikit-learn`, `matplotlib`, `seaborn`, ggf. `fairlearn` etc. installiert sind.

---

## Ablauf der Analyse

1. **Datenexploration & -bereinigung**  
   Untersuchung der Verteilungen, fehlende Werte und Inkonsistenzen in den Daten.

2. **Feature Engineering**  
   Extrahieren der Textmerkmale (z. B. Wortanzahl, bestimmte Keywords, Stilmetriken).

3. **Modelltraining & Bias-Messung**  
   Trainieren der Modelle zur Vorhersage (z. B. ob ein Lebenslauf â€žattractivâ€œ ist). Es werden anschlieÃŸend Fairness-Metriken wie Demographic Parity, Equalized Odds angewandt.

4. **Vergleich & Interpretation**  
   Analyse der Unterschiede zwischen Gruppen (z. B. Geschlecht) und Interpretation, welche Merkmale am stÃ¤rksten mit Abweichungen korrelieren.

5. **Visualisierung & Berichterstattung**  
   Erstellen von aussagekrÃ¤ftigen Grafiken und Summary der Ergebnisse in einem narrativen Bericht.

---

## Erkenntnisse

- Hohe Korrelation zwischen AI-Score und Hire-Entscheidung:
    Der AI-Score (0â€“100) ist fast direkt mit der Einstellung (Hire) verknÃ¼pft. Modelle, die diesen Score verwenden, erreichen deshalb extrem hohe Accuracy- und AUC-Werte, was auf eine mÃ¶gliche Label-Leakage oder Score-Kopplung hindeutet.

- Erfahrung wirkt sich stark positiv aus:
    Bewerber:innen mit mehr Berufsjahren werden deutlich hÃ¤ufiger als â€žHireâ€œ klassifiziert. Zwischen 0 und 8 Jahren steigt die Einstellungswahrscheinlichkeit fast monoton.

- Geschlechtsverteilung ausgewogen, aber geringe Stichprobe â€žunknownâ€œ:
    Etwa gleich viele mÃ¤nnliche und weibliche Profile, jedoch wenige â€žunknownâ€œ. Diese kleine Gruppe zeigt stÃ¤rkere Streuung in den Scores (hÃ¶here Varianz).

- Numerische Merkmale korrelieren unterschiedlich:
Erfahrung (Years) und AI-Score zeigen starke Korrelation (~0.8), wÃ¤hrend Projektezahl und Gehalt nur schwach beitragen.

- Kaum fehlende Werte oder Datenprobleme:
Nullraten liegen bei 0 % fÃ¼r alle zentralen Merkmale â€“ das Dataset ist vollstÃ¤ndig.

- Fairness-Metriken deuten geringe, aber messbare Unterschiede:
Die Analyse von Equalized Odds und Demographic Parity zeigt kleine Gaps zwischen Gruppen, besonders bei hÃ¶heren Thresholds (t > 0.5).

- Kalibrierung verbessert Fairness leicht:
Durch Isotonic bzw. Platt-Scaling wurden Score-Verteilungen Ã¼ber Gruppen angeglichen, was TPR/FPR-Differenzen reduzierte.

- Modelle ohne AI-Score verlieren stark an Performance:
Entfernt man den AI-Score als Feature, sinkt Accuracy deutlich â€“ Hinweis, dass der Score das dominante Merkmal ist.

- Tree-basierte Modelle (z. B. Gradient Boosting) liefern stabilere, aber weniger interpretierbare Ergebnisse als logistische Regression.

- Bootstrap- und Threshold-Analysen bestÃ¤tigen Robustheit:
Gruppenmetriken bleiben bei resampling stabil, allerdings nehmen Unterschiede bei extremen Schwellenwerten zu

---
